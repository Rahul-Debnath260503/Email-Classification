# -*- coding: utf-8 -*-
"""Email_classificatio.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NdtkkISqfkc3pElHZC_TwDlK6ioWDYo0

#Importing the Libraries
* "Pandas"      Used to read, process, and manipulate datasets efficiently.When working with structured data (CSV, Excel, etc.).Use it when we need to load, modify, and analyze tabular data.
  
* "Numpy"   Used for efficient mathematical operations on arrays.          When handling numerical data and performing mathematical operations.Use it when we need fast numerical computations.

* "Matlotlib.pyplot"  Used to create graphs and visualizations.When you need to analyze data visually.Use it when you need to plot graphs for analysis.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

"""#Load the Dataset"""

data = "/content/spam.csv"
df= pd.read_csv(data, encoding = "latin-1")

"""* viewing head to undderstand data"""

df.head()

from matplotlib import pyplot as plt
import seaborn as sns
df.groupby('Category').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))
plt.gca().spines[['top', 'right',]].set_visible(False)

df.count()

df['Category'] = df['Category'].map({'ham': 0, 'spam':1})

df['Message']= df['Message'].str.lower()

df.head()

import re
import nltk
from nltk.corpus import stopwords

'''Download stopwords if not available'''
nltk.download('stopwords')

stop_words = set(stopwords.words('english'))

# Function to clean text
def clean_text(text):
    text = re.sub(r'\W', ' ', text)  # Remove special characters
    text = re.sub(r'\s+', ' ', text)  # Remove extra spaces
    text = ' '.join([word for word in text.split() if word not in stop_words])  # Remove stopwords
    return text

# Apply text cleaning
df['Cleaned_Message'] = df['Message'].apply(clean_text)

# Display first few rows after cleaning
df.head()

# Define a basic list of common English stopwords (to avoid download issues)
basic_stopwords = set([
    "i", "me", "my", "myself", "we", "our", "ours", "ourselves", "you", "your", "yours",
    "he", "him", "his", "himself", "she", "her", "hers", "herself", "it", "its", "itself",
    "they", "them", "their", "theirs", "themselves", "what", "which", "who", "whom", "this",
    "that", "these", "those", "am", "is", "are", "was", "were", "be", "been", "being", "have",
    "has", "had", "having", "do", "does", "did", "doing", "a", "an", "the", "and", "but", "if",
    "or", "because", "as", "until", "while", "of", "at", "by", "for", "with", "about", "against",
    "between", "into", "through", "during", "before", "after", "above", "below", "to", "from",
    "up", "down", "in", "out", "on", "off", "over", "under", "again", "further", "then", "once",
    "here", "there", "when", "where", "why", "how", "all", "any", "both", "each", "few", "more",
    "most", "other", "some", "such", "no", "nor", "not", "only", "own", "same", "so", "than", "too", "very"
])

from sklearn.feature_extraction.text import TfidfVectorizer

# Convert text data into TF-IDF features
vectorizer = TfidfVectorizer(max_features=5000)  # Limit features to 5000 for efficiency
X = vectorizer.fit_transform(df['Cleaned_Message'])  # Transform text into numerical vectors

# Extract target labels
y = df['Category']

# Check shape of transformed data
X.shape, y.shape

from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
from sklearn.utils.class_weight import compute_class_weight
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report

# 1. Split data into training (80%) and testing (20%) sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# 2. Convert sparse matrix to dense for SMOTE
X_train_dense = X_train.toarray()

# 3. Apply SMOTE to balance classes
smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train_dense, y_train)

# 4. Train models on SMOTE-balanced data
# Naive Bayes
nb_model_smote = MultinomialNB()
nb_model_smote.fit(X_train_smote, y_train_smote)

# Logistic Regression
lr_model_smote = LogisticRegression(max_iter=1000)
lr_model_smote.fit(X_train_smote, y_train_smote)

# Decision Tree
dt_model_smote = DecisionTreeClassifier()
dt_model_smote.fit(X_train_smote, y_train_smote)

# 5. Convert X_test to dense for prediction
X_test_dense = X_test.toarray()

# 6. Predictions
nb_pred = nb_model_smote.predict(X_test_dense)
lr_pred = lr_model_smote.predict(X_test_dense)
dt_pred = dt_model_smote.predict(X_test_dense)

# 7. Accuracy and reports
print("Naive Bayes Accuracy:", accuracy_score(y_test, nb_pred))
print("Logistic Regression Accuracy:", accuracy_score(y_test, lr_pred))
print("Decision Tree Accuracy:", accuracy_score(y_test, dt_pred))

print("\nNaive Bayes Report:\n", classification_report(y_test, nb_pred))
print("Logistic Regression Report:\n", classification_report(y_test, lr_pred))
print("Decision Tree Report:\n", classification_report(y_test, dt_pred))

import matplotlib.pyplot as plt

# Accuracy scores
accuracies = [
    accuracy_score(y_test, nb_pred),
    accuracy_score(y_test, lr_pred),
    accuracy_score(y_test, dt_pred)
]

model_names = ['Naive Bayes', 'Logistic Regression', 'Decision Tree']

# Plotting
plt.figure(figsize=(8, 5))
plt.bar(model_names, accuracies, color=['skyblue', 'salmon', 'lightgreen'])
plt.title("Model Accuracy Comparison")
plt.ylabel("Accuracy")
plt.ylim(0.8, 1)
for i, acc in enumerate(accuracies):
    plt.text(i, acc + 0.005, f"{acc:.4f}", ha='center')
plt.show()

from sklearn.ensemble import VotingClassifier

# Create Voting Classifier (hard voting)
voting_clf = VotingClassifier(
    estimators=[
        ('nb', nb_model_smote),
        ('lr', lr_model_smote),
        ('dt', dt_model_smote)
    ],
    voting='hard'  # or 'soft' if using probability-based models
)

# Train on SMOTE data
voting_clf.fit(X_train_smote, y_train_smote)

# Predict on test data
voting_pred = voting_clf.predict(X_test_dense)

# Evaluate ensemble model
print("Voting Classifier Accuracy:", accuracy_score(y_test, voting_pred))
print("Voting Classifier Report:\n", classification_report(y_test, voting_pred))

import pickle

# Save the trained model
with open('spam_voting_model.pkl', 'wb') as f:
    pickle.dump(voting_clf, f)

# Save the vectorizer used for transforming text
with open('tfidf_vectorizer.pkl', 'wb') as f:
    pickle.dump(vectorizer, f)

!pip install streamlit

import streamlit as st
import pickle
import re
import nltk
from nltk.corpus import stopwords
import string

# Load model and vectorizer
with open('spam_voting_model.pkl', 'rb') as f:
    model = pickle.load(f)

with open('tfidf_vectorizer.pkl', 'rb') as f:
    vectorizer = pickle.load(f)

nltk.download('stopwords')

# Preprocess function
def preprocess(text):
    text = text.lower()
    text = re.sub(r'\d+', '', text)
    text = text.translate(str.maketrans('', '', string.punctuation))
    text = text.strip()
    words = text.split()
    words = [word for word in words if word not in stopwords.words('english')]
    return " ".join(words)

# App layout
st.title("ðŸ“§ Spam Email Classifier")

message = st.text_area("Enter the email text:")

if st.button("Predict"):
    if message.strip() == "":
        st.warning("Please enter a message to classify.")
    else:
        cleaned_msg = preprocess(message)
        msg_vec = vectorizer.transform([cleaned_msg])
        prediction = model.predict(msg_vec)

        if prediction[0] == 1:
            st.error("ðŸš« This is **SPAM**!")
        else:
            st.success("âœ… This is **NOT SPAM**.")